\chapter{Códigos Implementados}

A seguir são apresentados os algoritmos implementados para treinamento e validação das redes RBF.

\section{Classe RBFNetwork}

Esta seção descreve a classe `RBFNetwork`, que implementa uma Rede Neural de Funções de Base Radial (RBF). A classe possui métodos para inicializar a rede, calcular a camada oculta usando funções gaussianas, ajustar os pesos via aprendizado, prever resultados e avaliar a performance da rede.

\vspace{8pt}
\begin{lstlisting}[language=Python, caption={Classe RBFNetwork}]
from sklearn.preprocessing import StandardScaler
from scipy.spatial.distance import cdist
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

class RBFNetwork:
  def __init__(self, num_inputs, num_hidden, num_outputs):
    self.num_inputs = num_inputs
    self.num_hidden = num_hidden
    self.num_outputs = num_outputs
    self.mse_history = []
    self.centers = None
    self.weights = None
    self.scaler_x = None
    self.scaler_y = None
    self.sigma = None

  def _gaussian(self, x, c, s):
    return np.exp(-np.linalg.norm(x - c) ** 2 / (2 * s ** 2))

  def _calculate_hidden_layer(self, X):
    G = np.zeros((X.shape[0], self.num_hidden))
    for i in range(self.num_hidden):
      for j in range(X.shape[0]):
        G[j, i] = self._gaussian(X[j], self.centers[i], self.sigma)
    return G

  def _mean_squared_error(self, y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

  def reset(self):
    self.weights = None
    self.centers = None
    self.mse_history = []
    self.scaler_x = None
    self.scaler_y = None

  def fit(self, X, y, learning_rate=0.01, precision=1e-6, max_epochs=1000, seed=None):
    if seed:
      np.random.seed(seed)

    self.scaler_x = StandardScaler()
    X_scaled = self.scaler_x.fit_transform(X)

    self.scaler_y = StandardScaler()
    y_scaled = self.scaler_y.fit_transform(y.reshape(-1, 1)).flatten()

    kmeans = KMeans(n_clusters=self.num_hidden, random_state=seed)
    kmeans.fit(X_scaled)
    self.centers = kmeans.cluster_centers_

    distances = cdist(self.centers, self.centers, 'euclidean')
    d_max = np.max(distances)
    self.sigma = d_max / np.sqrt(2 * self.num_hidden)

    G = self._calculate_hidden_layer(X_scaled)

    self.weights = np.random.randn(self.num_hidden, self.num_outputs) * 0.1

    prev_error = float('inf')
    epoch = 0
    while True:
      y_hat = np.dot(G, self.weights).flatten()
      error = y_scaled - y_hat
      self.weights += learning_rate * np.dot(G.T, error.reshape(-1, 1))

      mse = self._mean_squared_error(y_scaled, y_hat)
      self.mse_history.append(mse)

      if np.abs(prev_error - mse) <= precision:
        break

      prev_error = mse
      epoch += 1
      if epoch == max_epochs:
        print("Aviso: Máximo de épocas alcançado sem convergência.")
        break

  def predict(self, X):
    X_scaled = self.scaler_x.transform(X)
    G = self._calculate_hidden_layer(X_scaled)
    y_pred_scaled = np.dot(G, self.weights).flatten()
    y_pred = self.scaler_y.inverse_transform(
        y_pred_scaled.reshape(-1, 1)).flatten()
    return y_pred

  def evaluate(self, X, y_true):
    predictions = self.predict(X)
    mse = self._mean_squared_error(y_true, predictions)
    return predictions, mse
\end{lstlisting}

\section{Experimentos}

Esta seção detalha os experimentos realizados para aproximação de funções utilizando a Rede Neural RBF. Os dados de treinamento e teste são carregados e, em seguida, são extraídas as variáveis de entrada e saída. Diferentes configurações de rede são testadas, variando o número de neurônios na camada oculta e as sementes para inicialização aleatória.

\vspace{8pt}
\begin{lstlisting}[language=Python, caption={Implementação dos experimentos.}]
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Leitura dos arquivos
df_train = pd.read_excel("6.5_RNA.xls")
df_test = pd.read_csv('6.5_RNA_test.txt', delim_whitespace=True)

# Exibição dos dados
display(df_train)
display(df_test)

# Extração das variáveis de entrada e saída
X_train = df_train[['x1', 'x2', 'x3']].values
y_train = df_train[['d']].values

X_test = df_test[['x1', 'x2', 'x3']].values
y_test = df_test[['d']].values

if __name__ == '__main__':

  rbf_params = [
      {'num_hidden': 5, 'seeds': [42, 89, 123]},
      {'num_hidden': 10, 'seeds': [42, 89, 123]},
      {'num_hidden': 15, 'seeds': [42, 89, 123]},
  ]

  for i, params in enumerate(rbf_params):
    num_hidden = params['num_hidden']
    seeds = params['seeds']

    best_mse = float('inf')
    best_history = None
    best_seed = None

    for j, seed in enumerate(seeds):
      print(f'Rede RBF-{i + 1} - T{j + 1}')

      # Criando uma única instância da RBF Network
      rbf_network = RBFNetwork(
          num_inputs=X_train.shape[1], num_hidden=num_hidden, num_outputs=1)

      # Reseta a rede neural radial para cada nova semente
      rbf_network.reset()

      # Treinando a rede com taxa de aprendizado de 0.01 e precisão de 1e-6
      rbf_network.fit(X_train, y_train, learning_rate=0.01,
                      precision=1e-6, seed=seed)

      prediction, mse = rbf_network.evaluate(X_test, y_test)

      print(
          f'Erro quadrático médio: {np.mean(rbf_network.mse_history):.3f}')
      print(f'epocas: {len(rbf_network.mse_history)}')

      print('Previsão=\n', prediction)
      print()
      print('MSE =', mse)
      print(
          f'Erro relativo medio: {100 * np.mean(np.abs(prediction - y_test.flatten()) / y_test.flatten()):.3f}')
      print(f'Variância: {np.var(prediction):.3f}')

      if mse < best_mse:
        best_mse = mse
        best_history = rbf_network.mse_history
        best_seed = seed

      print()

    # Plotando o erro quadrático medio em relação às epocas para o melhor treinamento
    plt.figure(figsize=(6, 3))
    plt.plot(best_history, linestyle='-', color='black', linewidth=1.5)
    plt.xlabel('epocas')
    plt.ylabel('MSE')
    plt.title(f'Histórico de MSE durante o Treinamento')
    plt.grid(linestyle='--')

    # Salvar em formato EPS
    plt.tight_layout()
    plt.savefig(
        f'report/figures/rbf-{i + 1}-best-training.eps', format='eps')
    plt.show()

    print(
        f'Melhor treinamento para RBF-{i + 1} foi com seed={best_seed} com MSE={best_mse:.3f}')
    print()
\end{lstlisting}

% ## Detalhamento dos Experimentos

% 1. **Configuração da Rede:**
% - A rede RBF foi configurada com 3 camadas: entrada, oculta e saída.
% - O número de neurônios na camada oculta variou entre 5, 10 e 15 para diferentes experimentos.
% - Três sementes diferentes (42, 89 e 123) foram usadas para a inicialização aleatória dos centros e pesos.

% 2. **Processamento dos Dados:**
% - Os dados de entrada foram padronizados usando `StandardScaler`.
% - A padronização foi aplicada tanto aos dados de entrada quanto aos dados de saída.

% 3. **Treinamento da Rede:**
% - O algoritmo de K-means foi utilizado para determinar os centros das funções gaussianas na camada oculta.
% - A largura das gaussianas (`sigma`) foi calculada com base na distância máxima entre os centros.
% - Os pesos da rede foram ajustados iterativamente até que a mudança no erro quadrático médio fosse menor que a precisão especificada (1e-6) ou o número máximo de épocas (1000) fosse alcançado.

% 4. **Avaliação da Rede:**
% - As previsões da rede foram comparadas com os valores reais dos dados de teste.
% - O desempenho da rede foi avaliado com base no erro quadrático médio

% (MSE) e no erro relativo médio.

% 5. **Resultados:**
% - O melhor desempenho foi identificado para cada configuração de rede com base na semente que resultou no menor MSE.
% - Foram gerados gráficos mostrando o histórico do MSE ao longo das épocas para a melhor semente de cada configuração.

% Este detalhamento adicional fornece uma visão completa da implementação e avaliação das redes RBF nos experimentos descritos.